<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xunzhe Zhou's Homepage</title>
    <meta name="author" content="Xunzhe Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        transition: background-color 0.3s ease, color 0.3s ease;
      }
      body.light-mode {
        background-color: #FFFFFF;
        color: #000;
      }
      body.dark-mode {
        background-color: #1F1F1F;
        color: #FFFFFF;
      }
      .name {
        text-align: center;
        font-size: xx-large;
        margin-bottom: 0;
      }
      table {
        width: 100%;
        max-width: 800px;
        border: 0;
        border-spacing: 0;
        border-collapse: separate;
        margin: auto;
      }
      td {
        padding: 2.5%;
      }
      img {
        width: 100%;
        max-width: 100%;
        object-fit: cover;
      }
      .hoverZoomLink {
        transition: transform 0.2s;
      }
      .hoverZoomLink:hover {
        transform: scale(1.04);
      }
      a {
        color: #007BFF;
        text-decoration: none;
        transition: color 0.3s ease;
      }
      body.dark-mode a {
        color: #80CFFF;
      }
      a:hover {
        animation: link-hover 0.3s ease forwards;
      }
      @keyframes link-hover {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #f09228;
          text-decoration: underline;
        }
      }
      body.dark-mode a:hover {
        animation: link-hover-dark 0.3s ease forwards;
      }

      @keyframes link-hover-dark {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #FFD700;
          text-decoration: underline;
        }
      }
      .news, .research, .publications, .projects {
        padding: 14px;
      }
      .footer {
        font-size: small;
        margin-top: 20px;
      }
      .theme-toggle {
        position: fixed;
        top: 10px;
        right: 10px;
        background-color: transparent;
        border: none;
        cursor: pointer;
      }
      .theme-toggle img {
        width: 40px;
        height: 40px;
        transition: transform 0.3s ease;
      }
    </style>
  </head>
  <body>
    <button class="theme-toggle" id="theme-toggle">
      <img src="assets/icon/sun.svg" id="theme-icon" alt="Toggle Theme">
    </button>
    <table>
      <tr>
        <td>
          <table>
            <tr>
              <td style="width: 60%; vertical-align: middle;">
                <p class="name">Xunzhe Zhou</p>
                <p>
                  I am an undergraduate student at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>, pursuing my Bachelor's degree in Computer Science and Technology.
                </p>
                <p>
                  Recently, I have been an intern at <a href="https://www.comp.nus.edu.sg/">NUS</a> working with Prof. <a href="https://linsats.github.io/">Lin Shao</a> studying robotic task and motion planning. Previously, I collaborated with Prof. <a href="https://faculty.fudan.edu.cn/xyxue/zh_CN/index.htm">Xiangyang Xue</a> and Prof. <a href="https://yanweifu.github.io/">Yanwei Fu</a> on constructing a mobile manipulation system with 3D reconstruction and vision-language models. I also worked with Prof. <a href="http://kw.fudan.edu.cn/people/xiaoyanghua/">Yanghua Xiao</a> on studying LLMs' hard-constraint instruction following capabilities, and with Prof. <a href="https://faculty.fudan.edu.cn/syleng/zh_CN/index.htm">Siyang Leng</a> on nonlinear dynamical systems control.
                </p>
                <p>
                  I also had a wonderful semester at the <a href="https://www.berkeley.edu/">UC Berkeley</a> during fall 2023, where I studied reinforcement learning, deep learning, optimization models, and artificial intelligence, with GPA 4.00 / 4.00.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xunzhe_zhou@outlook.com">Email</a> &nbsp;/&nbsp;
                  <a href="./assets/files/CV_202407.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Zhouxunzhe">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=IEi7AToAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xunzhe-zhou/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/ZHOUxzhe">Twitter</a>
                </p>
              </td>
              <td style="width: 40%; max-width: 40%;">
                <a href="assets/img/berkeley.jpg">
                  <img alt="profile photo" src="assets/img/berkeley.jpg" style="width:100%;max-width:100%;object-fit: cover;" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </table>

          <div class="news">
            <h2>News</h2>
            <p>[Dec. 2023] Our paper <strong>CELLO</strong> is accepted by AAAI 2024!</p>
          </div>

          <div class="research">
            <h2>Research</h2>
            <p>
              My ultimate goal is to enable robots to assist human with daily task automatically. 
            </p>
            <p>
              My current research interests focus on leveraging existing foundational models to assist robots with perception and decision-making in the physical world. Specifically, <br>1) I aim to develop a general perception system to understand complex and dynamical environments; <br>2) I seek to create a general decision-making systems for real-world long-term task planning.
            </p>
            <p>
              In the future, I plan to continue my research work with a unified lifelong learning system by <br>1) developing a data collection system in different scenarios with various tasks, <br>2) constructing a universal learning system for heterogeneous robots to learn unified skills, and <br>3) building a collaborative system to enable real robots complete daily task more efficiently.
            </p>
          </div>

          <div class="publications">
            <h2>Publications</h2>
            <p>
              <!-- Representative papers are <span class="highlight">highlighted</span>.  -->
              * denotes equal contribution.
            </p>
            <table>
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="assets/img/2024_rc.png" alt="RC" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">Reservoir computing as digital twins for controlling nonlinear dynamical systems</span></strong></a><br>
                  R. Cao*, <u><strong>Xunzhe Zhou*</strong></u>, J. Hou, C. Guan, S. Leng<br>
                  <em>In submission</em>, 2024<br>
                  <!-- <a href="assets/abstract/2024_rc.txt">abstract</a> -->
                  <p>We validate the effectiveness of our digital twins in replicating unknown complex systems and further demonstrate how to select and implement appropriate control strategies based on the specific requirements.</p>
                </td>
              </tr>
              <!-- <tr bgcolor="#ffffd0"> -->
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="./assets/img/2023_cello.png" alt="CELLO" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">Can Large Language Models Understand Real-World Complex Instructions?</span></strong></a><br>
                  Q. He, J. Zeng, W. Huang, L. Chen, J. Xiao, Q. He, <u><strong>Xunzhe Zhou</strong></u>, J. Liang, Y. Xiao<br>
                  <em>AAAI</em>, 2024<br>
                  <a href="https://abbey4799.github.io/publication/cello/">project page</a> / <a href="assets/abstract/2023_cello.txt">abstract</a> / <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29777">paper</a> / <a href="https://github.com/Abbey4799/CELLO">code</a> / <a href="assets/bibtex/2023_cello.bib">bibtex</a> / <a href="https://underline.io/lecture/92662-can-large-language-models-understand-real-world-complex-instructionsquestion">video</a>
                  <p>We propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and corresponding metrics.</p>
                </td>
              </tr>
            </table>
          </div>

          <div class="projects">
            <h2>Selected Projects</h2>
            <table>
                <tr>
                    <td style="width: 30%; vertical-align: middle;">
                    <img src="assets/img/2024_tamma.gif" alt="NST" width="200">
                    </td>
                    <td style="width: 70%; vertical-align: middle;">
                    <a><strong><span class="papertitle">Target-driven Multi-subscene Mobile Manipulation</span></strong></a><br>
                    <!-- Participate in the system construction.</br> -->
                    Contribute to the system construction. Paper authored by Hou et al. is accepted by <em>CoRL</em>, 2024<br>
                    <a href="https://jarvishou829.github.io/TaMMa/">project page</a> / <a href="assets/abstract/2024_tamma.txt">abstract</a> / <a href="https://openreview.net/forum?id=EiqQEsOMZt&referrer=%5Bthe%20profile%20of%20Yanwei%20Fu%5D(%2Fprofile%3Fid%3D~Yanwei_Fu2)">paper</a> / <a>code (coming soon)</a>
                    <p>TAMMA is a multi-subscene mobile manipulation approach. Coarse scene prior and refined pose estimation empower the mobile base movement and robotic arm manipulation effectively. I mainly participate in the preliminary work of this project.</p>
                    </td>
                </tr>
                <tr>
                    <td style="width: 30%; vertical-align: middle;">
                    <img src="assets/img/2023_nst.png" alt="NST" width="200">
                    </td>
                    <td style="width: 70%; vertical-align: middle;">
                    <a><strong><span class="papertitle">Neural Style Transfer Based on Fine Tuning Vision Transformer</span></strong></a><br>
                    <em>UC Berkeley CS182/282A course project</em>, 2023<br>
                    <a href="https://github.com/Zhouxunzhe/NST-NeuralStyleTransfer">project page</a> / <a href="assets/files/NST.pdf">essay</a> / <a href="https://github.com/Zhouxunzhe/NST-NeuralStyleTransfer">code</a>
                    <p>Inspired by StyTr<sup>2</sup>, we replace its content and style encoders with pretrained ViT models. We propose a two-stage training strategy and wrap the ViTs with LoRA for joint training.</p>
                    </td>
                </tr>
            </table>
          </div>

          <div class="footer">
            <p style="text-align: center">Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.</p>
          </div>
        </td>
      </tr>
    </table>

    <script>
        const userPrefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        const currentTheme = localStorage.getItem('theme') ||
          (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark-mode' : 'light-mode');
        document.documentElement.classList.add(currentTheme);
        document.documentElement.style.backgroundColor = currentTheme === 'dark-mode' ?  '#1F1F1F': '#FFFFFF';
        
        document.documentElement.style.visibility = 'hidden';
        window.addEventListener('load', () => {
          document.body.classList.add(currentTheme);
          document.documentElement.style.visibility = 'visible';
        });
  
        const themeIcon = document.getElementById('theme-icon');
        themeIcon.src = currentTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
  
        const themeToggleButton = document.getElementById('theme-toggle');
  
        themeToggleButton.addEventListener('click', () => {
          document.body.classList.toggle('dark-mode');
          document.body.classList.toggle('light-mode');
  
          const newTheme = document.body.classList.contains('dark-mode') ? 'dark-mode' : 'light-mode';
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
          themeIcon.style.transform = 'rotate(360deg)';
          setTimeout(() => themeIcon.style.transform = '', 300);
        });
  
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', event => {
          const newTheme = event.matches ? 'dark-mode' : 'light-mode';
          document.body.classList.add(newTheme);
          document.body.classList.remove(event.matches ? 'light-mode' : 'dark-mode');
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
        });
      </script>
  </body>
</html>